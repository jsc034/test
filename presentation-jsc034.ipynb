{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Math 157: Intro to Mathematical Software\n",
    "## UC San Diego, Winter 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Final project, part 2: Joshua Cho- Model Selection for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following notebook will be using ideas presented in the Model Selection section on the scikit learn website. http://scikit-learn.org/stable/model_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Model Selection: The Big Picture\n",
    "Suppose you had a big midterm coming up and want to get a good score. Lucky for you, the professor has also provided a practice midterm to prepare you for the exam. You study only the material on the practice exam because you reason that the real exam will test topics similar to the practice exam. On exam day you recieve the midterm only to find out that it is the exact same test as the practice midterm. Having studied the practice midterm, you ace the test with a perfect score. However did the exam really test you on how much you learned? Real exams test you to see if you understand the concepts of the class by testing you on questions similar to the material, but most importantly, questions you have never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Similar to the midterm example above, model selection is a test to measure the predictive power on unforseen data. Like the practice midterm, if the machine studied the training data and was then tested on the same training data, it would ace the test, but that's not useful in the real world. Like students, the machine learns from the testing data and then predicts on some new data it has never seen before as a measure of how 'well' it has learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Model Selection: Getting Started\n",
    "For this presentation, I will be focusing on supervised Machine Learning. Supervised learning means that you have the labels of the training data whereas unsupervised learning means that you no have the labels for your training data. Following the midterm example, supervised learning would mean you are given the practice midterm (training data) and the solutions to each probelm (the desired labels). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since we are in the setting of supervised learning, our training data is made of 2 components: the many variables of the dataset and the corresponding labels. In this way, you can think of each model as a function that takes the inputs of the dataset and returns a label according to the function. We have X and Y, but we need to select the best function f. Since we are only given the training data and do not know what the real data will be, we split the training data into a training set and testing set. Then we procede to train our model on the smaller training set and predict on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Leave One Out Cross Validation (LOOCV)\n",
    "We will start with leave-one-out because it is relatively simple. We have our dataset of $n$ samples: we have n pairs of labels and inputs. Now imagine that we did not have the 1st label. Then we could train a model on the remaining $n-1$ data points to predict the 1st label. Now imagine that we did not have the 2nd label. Then we could train a model on the remaining $n-1$ data points to predict the 2nd label. We can repeat the procedure $n$ times, each time pretending we do not have the label of the i-th data point and using the remaning $n-1$ data points to train a model and predict the missing label. Recall that in reality, we do have the value for the label we pretended to be 'missing' so we can compare the real label to the predicted label. We can then use the accuracy as a measure of the predictive power of our model.\n",
    "\n",
    "Source: https://www.cs.cmu.edu/~schneide/tut5/node42.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 1,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "iris.data.shape, iris.target.shape\n",
    "#notice that we have 150 data points, each with 4 features and 150 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "#Here we perform LOOCV on the iris dataset, meaning we loop 150 times\n",
    "LOOCV = LeaveOneOut()\n",
    "scores = np.zeros(len(iris.data)); count = 0\n",
    "for index_train, index_test in LOOCV.split(iris.data):\n",
    "    trainX = iris.data[index_train]\n",
    "    testX = iris.data[index_test]\n",
    "    trainY = iris.target[index_train]\n",
    "    testY = iris.target[index_test]\n",
    "\n",
    "    #we train on the 149 data points and guess the missing label and see if the prediction is accurate or not\n",
    "    clf = svm.SVC(kernel='linear', C=1).fit(trainX,trainY)\n",
    "    scores[count] = clf.score(testX,testY)\n",
    "    count+=1\n",
    "\n",
    "#an array of accuracies\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Above we get 150 different estimators, each made from exclusing 1 point from the avaliable 150. We see an overwhelming majority of the estimators managed to predict the missing label correctly.\n",
    "\n",
    "Notice that for large n, this method becomes very computationally infeasible. In addition, for large n, your training set that you make from the $n-1$ is indistinguishable from the entire set so it is like you are training on the entire dataset anyways. We need a smarter method that captures the idea of LOOCV but is computationally more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cross Validation\n",
    "Since having n groups would be computationally infeasible, let's instead split the given data into 2 groups- with $p$ being the percentage of the given data that you want use for training. Then the size of the training set is $np$ and the size of the testing set is $n(1-p)$. Since we want our test model to be accurate, we want to feed it a larger partition of the data we have avalaible, thus we pick a $p\\geq 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((82, 4), (82,))"
      ]
     },
     "execution_count": 3,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "\n",
    "#Splitting the avaliable data into a training set and testing set\n",
    "p = .55\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=(1-p))\n",
    "X_train.shape, y_train.shape\n",
    "\n",
    "#notice that the training set is 150*.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((68, 4), (68,))"
      ]
     },
     "execution_count": 4,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#and the testing set is 150*(1-.55)\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98529411764705888"
      ]
     },
     "execution_count": 7,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build a model on the training set, then pass the testing set into the constructed model and return it's accuracy score\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Look! We get a high accuracy score! A job well done and we can stop right? Well, not quite. \n",
    "\n",
    "Since we randomly chose which data went into training and testing it could be the fact that we got quite lucky and we split the data nicely. This in turn might have cause the estimator to overfit- that is it learned so well from the training data that it does a bad job predicting new data it has never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### K-fold Cross Validation\n",
    "So now we have learned that choosing n groups is too computationally expensive and choosing 2 groups can lead to overfitting. This leads us to K-fold Cross Validation which expands on the idea of LOOCV by forming $k$ groups as opposed to $n$ groups. Here $k$ is an integer which represents the number of groups you want to divide the dataset into, each with $\\frac{n}{k}$ elements randomly assigned. Then you pretend the i-th group is your testing data and your train on the remaining $k-1$ groups and get the accuracy. You repeat the process k times and see which model returns the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.94117647,  0.91176471,  0.97058824,  0.98529412])"
      ]
     },
     "execution_count": 8,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Splitting the avaliable data into a k groups, then letting one group be the testing data and the remaining k-1 groups to be the training set. This procedure is repeated for each group as the testing data\n",
    "p = .55\n",
    "k = 5\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "folds = ShuffleSplit(n_splits = k, test_size = (1-p))\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv = folds)\n",
    "scores #returns k values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since all the scores are similar, you can be sure that overfitting is not happening and that the estimator is doing a good job at predicitng the labels given this training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 1: Iris Dataset\n",
    "Below is a copy of the code above for testing the cross validation of the iris dataset. Pick a  $p \\leq 0.5$. Explain why the score is still pretty good for $p \\leq 0.5$. Hint: Do some research on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97058823529411764"
      ]
     },
     "execution_count": 9,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "\n",
    "p = .55\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=(1-p))\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Your reponse here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 2: K-fold CV\n",
    "Perform K-fold Cross Validation on the digits dataset below with $p=0.7$ and $k=10$ using the K-Nearest Neighbors estimator. Get the average score from the K-fold Cross Validation of the training set and see if score of the final testing data trained on the entire training set is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 10,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "digits.data.shape, digits.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (1437,))"
      ]
     },
     "execution_count": 11,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX, final_testX, dataY, final_testY = train_test_split(digits.data, digits.target, test_size=.2)\n",
    "dataX.shape, dataY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "          metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "execution_count": 12,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do not forget to import the estimator\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#K-Nearest Neighbors estimator using 9 of its neighbors\n",
    "KNeighborsRegressor(n_neighbors=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />\n",
    "~ <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Solutions\n",
    "\n",
    "Exercise 1: Source: https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "\n",
    "The Iris dataset is a famous dataset because of how easily the 3 categories are sperated using most machine learning techniques. A $p \\leq .5$ means that our training sample is smaller which normally is a bad choice due to the weak classifier built. However, since these 3 categories are so distinct, a classifier trained a fewer points is still able to predict the correct labels with high accuracy.\n",
    "\n",
    "Exercise 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.938392014825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.96406766511502406"
      ]
     },
     "execution_count": 13,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "digits.data.shape, digits.target.shape\n",
    "\n",
    "dataX, final_testX, dataY, final_testY = train_test_split(digits.data, digits.target, test_size=.2)\n",
    "dataX.shape, dataY.shape\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor #need to import\n",
    "\n",
    "p = .7\n",
    "k = 10\n",
    "clf = KNeighborsRegressor(n_neighbors=9) #change the estimator\n",
    "folds = ShuffleSplit(n_splits = k, test_size = (1-p))\n",
    "scores = cross_val_score(clf, dataX, dataY, cv = folds)\n",
    "print(mean(scores))\n",
    "\n",
    "clf = KNeighborsRegressor(n_neighbors=9).fit(dataX,dataY)\n",
    "clf.score(final_testX,final_testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 8.1",
   "name": "sage-8.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}